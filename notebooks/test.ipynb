{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T01:06:52.458132700Z",
     "start_time": "2024-04-02T01:06:52.449281200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import AdamW\n",
    "from keras.optimizers import Nadam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dir_path = \"../\"\n",
    "data_path = \"../data\"\n",
    "output_path = \"../outputs\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T01:06:54.385428600Z",
     "start_time": "2024-04-02T01:06:54.380179400Z"
    }
   },
   "id": "79b77dcc6edba2bb",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0acde728f534c5e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T01:08:58.682231300Z",
     "start_time": "2024-04-02T01:08:58.642050700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Dropout, Lambda\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras import regularizers\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32 #* strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "\n",
    "def build_bert_model(max_len=192, optimizer = Adam()):\n",
    "  \"\"\"\n",
    "  That function create the BERT model for training\n",
    "  \"\"\"\n",
    "  # Charger le modèle pré-entraîné DistilBERT et le tokenizer\n",
    "  distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "  #tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "    # La couche d'entrée\n",
    "    Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"),\n",
    "\n",
    "    # Ajouter la couche DistilBERT (notez que nous utilisons distilbert_model.layers[0] pour accéder à la couche de transformer)\n",
    "    # La couche DistilBERT\n",
    "    distilbert_model.layers[0],\n",
    "\n",
    "    # La couche pour obtenir le premier token [CLS]\n",
    "    Lambda(lambda seq: seq[:, 0, :]),\n",
    "\n",
    "    #Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "    #Dropout(0.5),\n",
    "    #Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "    #Dropout(0.5),\n",
    "    #Dense(64,  activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "\n",
    "    # Ajouter des couches supplémentaires si nécessaire\n",
    "    # Par exemple, une couche Dense pour la classification\n",
    "    # La couche de sortie\n",
    "    Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  loss = BinaryCrossentropy()\n",
    "  #metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "  # Compiler le modèle\n",
    "  # Compiler le modèle avec une loss adaptée à la classification binaire\n",
    "  model.compile(optimizer = optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "  # Afficher le résumé du modèle\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd57d726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:09:20.226290600Z",
     "start_time": "2024-04-02T01:09:17.757198100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Received: layer=<KerasTensor shape=(None, 192), dtype=int32, sparse=None, name=input_word_ids> of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m new_model \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_bert_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Créez le modèle avec la même architecture\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#new_model.load_weights(output_path+'/bert_model-5-val.h5') \u001B[39;00m\n",
      "Cell \u001B[1;32mIn[9], line 18\u001B[0m, in \u001B[0;36mbuild_bert_model\u001B[1;34m(max_len, optimizer)\u001B[0m\n\u001B[0;32m     15\u001B[0m distilbert_model \u001B[38;5;241m=\u001B[39m TFDistilBertModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdistilbert-base-multilingual-cased\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# La couche d'entrée\u001B[39;49;00m\n\u001B[0;32m     20\u001B[0m \u001B[43m  \u001B[49m\u001B[43mInput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmax_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint32\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_word_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Ajouter la couche DistilBERT (notez que nous utilisons distilbert_model.layers[0] pour accéder à la couche de transformer)\u001B[39;49;00m\n\u001B[0;32m     23\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# La couche DistilBERT\u001B[39;49;00m\n\u001B[0;32m     24\u001B[0m \u001B[43m  \u001B[49m\u001B[43mdistilbert_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# La couche pour obtenir le premier token [CLS]\u001B[39;49;00m\n\u001B[0;32m     27\u001B[0m \u001B[43m  \u001B[49m\u001B[43mLambda\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mseq\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m#Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\u001B[39;49;00m\n\u001B[0;32m     30\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m#Dropout(0.5),\u001B[39;49;00m\n\u001B[0;32m     31\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m#Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\u001B[39;49;00m\n\u001B[0;32m     32\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m#Dropout(0.5),\u001B[39;49;00m\n\u001B[0;32m     33\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m#Dense(64,  activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\u001B[39;49;00m\n\u001B[0;32m     34\u001B[0m \u001B[43m\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Ajouter des couches supplémentaires si nécessaire\u001B[39;49;00m\n\u001B[0;32m     36\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Par exemple, une couche Dense pour la classification\u001B[39;49;00m\n\u001B[0;32m     37\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# La couche de sortie\u001B[39;49;00m\n\u001B[0;32m     38\u001B[0m \u001B[43m  \u001B[49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msigmoid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m loss \u001B[38;5;241m=\u001B[39m BinaryCrossentropy()\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m#metrics = tf.metrics.BinaryAccuracy()\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Compiler le modèle\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Compiler le modèle avec une loss adaptée à la classification binaire\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\nlp_toxic_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 204\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\nlp_toxic_detection\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\PycharmProjects\\nlp_toxic_detection\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\sequential.py:177\u001B[0m, in \u001B[0;36mSequential.add\u001B[1;34m(self, layer)\u001B[0m\n\u001B[0;32m    175\u001B[0m         layer \u001B[38;5;241m=\u001B[39m functional\u001B[38;5;241m.\u001B[39mModuleWrapper(layer)\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    178\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe added layer must be an instance of class Layer. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived: layer=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(layer)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    180\u001B[0m     )\n\u001B[0;32m    182\u001B[0m tf_utils\u001B[38;5;241m.\u001B[39massert_no_legacy_layers([layer])\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_layer_name_unique(layer):\n",
      "\u001B[1;31mTypeError\u001B[0m: The added layer must be an instance of class Layer. Received: layer=<KerasTensor shape=(None, 192), dtype=int32, sparse=None, name=input_word_ids> of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'>."
     ]
    }
   ],
   "source": [
    "new_model = build_bert_model()  # Créez le modèle avec la même architecture\n",
    "#new_model.load_weights(output_path+'/bert_model-5-val.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a9a61",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-02T01:01:06.161585200Z"
    }
   },
   "outputs": [],
   "source": [
    "examples1 = [\n",
    "    \"malade\",\n",
    "    \"Je suis malade\",\n",
    "    \"Je suis gravement malade\",\n",
    "    \"Je suis malade, j'ai pris un medicament\",\n",
    "    \"Je suis malade, j'ai pris un medicament, je vais bien maintenant\",\n",
    "]\n",
    "\n",
    "examples0 = [\n",
    "    'Everythings goes bad today, but i have to smile, that\\'s make me happy',\n",
    "    'I am tired',\n",
    "    'I am not healthy',\n",
    "    'I am sick',\n",
    "    'I am gonna kill you',\n",
    "    'I am angry about you bastard',\n",
    "    'Love you',\n",
    "    'Fuck you',\n",
    "]\n",
    "\n",
    "examples2 = [\n",
    "    'Je suis heureux',\n",
    "    'Je t\\'aime',\n",
    "    'Je te haie',\n",
    "    'Va chier',\n",
    "    'Va dormir',\n",
    "    'Te quiero mucho',\n",
    "    'Como estas',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e879a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:01:06.164823700Z",
     "start_time": "2024-04-02T01:01:06.164823700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "\n",
    "# Phrase à tester\n",
    "# First load the real tokenizer\n",
    "try:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(output_path+'/tokenizers')\n",
    "except (OSError, ValueError):\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "    # Save the loaded tokenizer locally\n",
    "    tokenizer.save_pretrained(output_path+'/tokenizers')\n",
    "\n",
    "def test_toxic(phrase):\n",
    "  # Prétraiter la phrase\n",
    "  inputs = tokenizer(phrase, return_tensors=\"tf\", max_length=MAX_LEN, truncation=True, padding='max_length')\n",
    "\n",
    "  # Obtenir la prédiction\n",
    "  predictions = new_model.predict(inputs['input_ids'])\n",
    "\n",
    "  # Interpréter la prédiction\n",
    "  toxic_threshold = 0.5  # Définir un seuil\n",
    "  is_toxic = predictions[0, 0] > toxic_threshold\n",
    "\n",
    "  per = predictions[0, 0] * 100\n",
    "\n",
    "  print(f\" **{phrase}** a une toxicité de {per:.2f} - [{'toxique' if is_toxic else 'non-toxique'}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b74c4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-02T01:01:06.168381200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c56e26",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-02T01:01:06.170555300Z"
    }
   },
   "outputs": [],
   "source": [
    "for phrase in examples1:\n",
    "  test_toxic(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b5169",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-02T01:01:06.187500800Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
