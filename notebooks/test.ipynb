{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import AdamW\n",
    "from keras.optimizers import Nadam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0acde728f534c5e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32 #* strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "\n",
    "def build_bert_model(max_len=192, optimizer = Adam()):\n",
    "  \"\"\"\n",
    "  That function create the BERT model for training\n",
    "  \"\"\"\n",
    "  # Charger le modèle pré-entraîné DistilBERT et le tokenizer\n",
    "  distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "  #tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "    # La couche d'entrée\n",
    "    Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"),\n",
    "\n",
    "    # Ajouter la couche DistilBERT (notez que nous utilisons distilbert_model.layers[0] pour accéder à la couche de transformer)\n",
    "    # La couche DistilBERT\n",
    "    distilbert_model.layers[0],\n",
    "\n",
    "    # La couche pour obtenir le premier token [CLS]\n",
    "    tf.keras.layers.Lambda(lambda seq: seq[:, 0, :]),\n",
    "\n",
    "    #Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "    #Dropout(0.5),\n",
    "    #Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "    #Dropout(0.5),\n",
    "    #Dense(64,  activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "\n",
    "    # Ajouter des couches supplémentaires si nécessaire\n",
    "    # Par exemple, une couche Dense pour la classification\n",
    "    # La couche de sortie\n",
    "    Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  loss = tf.keras.losses.BinaryCrossentropy()\n",
    "  #metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "  # Compiler le modèle\n",
    "  # Compiler le modèle avec une loss adaptée à la classification binaire\n",
    "  model.compile(optimizer = optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "  # Afficher le résumé du modèle\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd57d726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "new_model = build_bert_model()  # Créez le modèle avec la même architecture\n",
    "new_model.load_weights('outputs/bert_model-5-val.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf7a9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples1 = [\n",
    "    \"malade\",\n",
    "    \"Je suis malade\",\n",
    "    \"Je suis gravement malade\",\n",
    "    \"Je suis malade, j'ai pris un medicament\",\n",
    "    \"Je suis malade, j'ai pris un medicament, je vais bien maintenant\",\n",
    "]\n",
    "\n",
    "examples0 = [\n",
    "    'Everythings goes bad today, but i have to smile, that\\'s make me happy',\n",
    "    'I am tired',\n",
    "    'I am not healthy',\n",
    "    'I am sick',\n",
    "    'I am gonna kill you',\n",
    "    'I am angry about you bastard',\n",
    "    'Love you',\n",
    "    'Fuck you',\n",
    "]\n",
    "\n",
    "examples2 = [\n",
    "    'Je suis heureux',\n",
    "    'Je t\\'aime',\n",
    "    'Je te haie',\n",
    "    'Va chier',\n",
    "    'Va dormir',\n",
    "    'Te quiero mucho',\n",
    "    'Como estas',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3e879a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "\n",
    "# Phrase à tester\n",
    "# First load the real tokenizer\n",
    "try:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained('outputs/tokenizers')\n",
    "except (OSError, ValueError):\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "    # Save the loaded tokenizer locally\n",
    "    tokenizer.save_pretrained('outputs/tokenizers')\n",
    "\n",
    "def test_toxic(phrase):\n",
    "  # Prétraiter la phrase\n",
    "  inputs = tokenizer(phrase, return_tensors=\"tf\", max_length=MAX_LEN, truncation=True, padding='max_length')\n",
    "\n",
    "  # Obtenir la prédiction\n",
    "  predictions = new_model.predict(inputs['input_ids'])\n",
    "\n",
    "  # Interpréter la prédiction\n",
    "  toxic_threshold = 0.5  # Définir un seuil\n",
    "  is_toxic = predictions[0, 0] > toxic_threshold\n",
    "\n",
    "  per = predictions[0, 0] * 100\n",
    "\n",
    "  print(f\" **{phrase}** a une toxicité de {per:.2f} - [{'toxique' if is_toxic else 'non-toxique'}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "080b74c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5c56e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      " **malade** a une toxicité de 0.18 - [non-toxique]\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      " **Je suis malade** a une toxicité de 99.11 - [toxique]\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      " **Je suis gravement malade** a une toxicité de 95.69 - [toxique]\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      " **Je suis malade, j'ai pris un medicament** a une toxicité de 95.88 - [toxique]\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      " **Je suis malade, j'ai pris un medicament, je vais bien maintenant** a une toxicité de 99.66 - [toxique]\n"
     ]
    }
   ],
   "source": [
    "for phrase in examples1:\n",
    "  test_toxic(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b5169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
